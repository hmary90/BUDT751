The fraud detection system was built using a comprehensive unsupervised learning pipeline on transactional banking data. Here are the key components:

📊 **Data Overview:**

* Dataset: Banking transactions from customers at Sterling Oak Bank
* Rows: \~10,000+
* Columns included:
 - TransactionID: Unique alphanumeric identifier for each transaction.
 - AccountID: Unique identifier for each account, with multiple transactions per account.
 - TransactionAmount: Monetary value of each transaction, ranging from small everyday expenses to larger purchases.
 - TransactionDate: Timestamp of each transaction, capturing date and time.
 - TransactionType: Categorical field indicating 'Credit' or 'Debit' transactions.
 - Location: Geographic location of the transaction, represented by U.S. city names.
 - DeviceID: Alphanumeric identifier for devices used to perform the transaction.
 - IP Address: IPv4 address associated with the transaction, with occasional changes for some accounts.
 - MerchantID: Unique identifier for merchants, showing preferred and outlier merchants for each account.
 - AccountBalance: Balance in the account post-transaction, with logical correlations based on transaction type and amount.
 - PreviousTransactionDate: Timestamp of the last transaction for the account, aiding in calculating transaction frequency.
 - Channel: Channel through which the transaction was performed (e.g., Online, ATM, Branch).
 - CustomerAge: Age of the account holder, with logical groupings based on occupation.
 - CustomerOccupation: Occupation of the account holder (e.g., Doctor, Engineer, Student, Retired), reflecting income patterns.
 - TransactionDuration: Duration of the transaction in seconds, varying by transaction type.
 - LoginAttempts: Number of login attempts before the transaction, with higher values indicating potential anomalies.

* Initial exploration checked for missing data, duplicates, and statistical summaries

🛠️ **Feature Engineering:**

* Extracted temporal features:

  * `Hour`, `DayOfWeek`, `Weekend` (1 if Sat/Sun), `Month`
* Time-based behavioral feature:

  * `TimeSinceLastTx` (in hours)
* Statistical outlier detection:

  * Z-score computed on TransactionAmount
* Ratio features:

  * `Amount_to_AvgByType_Ratio` = TransactionAmount / Avg for that TransactionType
* Behavioral count feature:

  * `DeviceTxCount` = total transactions per DeviceID

🧹 **Preprocessing:**

* Dropped non-model-useful fields: TransactionID, AccountID, IP Address, date columns
* Label-encoded categorical variables (e.g., Channel, TransactionType)
* StandardScaler applied to all numeric features
* Final dataset shape: \~10–15 numerical columns after processing

🧠 **Unsupervised Models Used:**

1. **Isolation Forest:**

   * Grid search performed for tuning
   * Best params: `n_estimators=200`, `contamination=0.02`, `max_samples='auto'`
   * Outputs:

     * `IF_Anomaly`: 1 if predicted as anomaly, 0 otherwise
     * `IF_Score`: distance from anomaly boundary

2. **DBSCAN:**

   * Used KNN distance plot to tune `eps`
   * Best params found: `eps=2.0`, `min_samples=10`
   * Outputs:

     * `DBSCAN_Label`: cluster number or -1 for anomaly
     * `DBSCAN_Anomaly`: 1 if label is -1, else 0

3. **One-Class SVM:**

   * Kernel: RBF
   * `nu=0.05`, `gamma='auto'`
   * Outputs:

     * `OCSVM_Anomaly`: 1 if predicted as anomaly, 0 otherwise
     * `OCSVM_Score`: signed distance from the separating hyperplane

🧩 **Ensemble Model Logic:**

* Target variable: `Ensemble_Anomaly`
* Defined as 1 (fraud) if **2 or more models** predict an anomaly
* Combines predictions from `IF_Anomaly`, `DBSCAN_Anomaly`, and `OCSVM_Anomaly`

📊 **Visual & Statistical Analysis:**

* Compared fraud vs. non-fraud across:

  * `TransactionAmount`, `Hour`, `Channel`, `TransactionType`
* Found frauds more likely:

  * At night, during weekends
  * Through certain channels (e.g., web/mobile)
* Outlier transactions had higher Z-scores

🌲 **Decision Tree Model (for rule explanation):**

* Trained to learn rules from ensemble-labeled data
* Max depth = 5
* Used `plot_tree()` and `export_text()` to extract human-readable rules
* Example rule:

  * If `TransactionAmount > 9000` and `Hour > 20` and `Channel = Mobile`, then high fraud likelihood

🔥 **Feature Importance (Random Forest):**

* Most important features:

  1. `TransactionAmount`
  2. `Hour`
  3. `Channel`
  4. `TransactionType`
  5. `DayOfWeek`
  6. `Amount_to_AvgByType_Ratio`
  7. `DeviceTxCount`

📈 **Evaluation & Agreement Analysis:**

*Fradulent Transactions Found:
 * 110 flagged transactions
 * Ensemble method used to create the list of flagged transactions

 Summary of Fraud Detection Rules:
 
 The following decision rules are used to identify transactions as potentially fraudulent:
 
 1. When LoginAttempts <= 3.5:
    - If TransactionAmount > 959.19 and MerchantID <= 4.5 → classify as fraud.
    - If TransactionDuration > 262.5:
      - If TransactionType <= 0.5 and Location > 27 → classify as fraud.
      - If TransactionType > 0.5 and MerchantID > 97.5 → classify as fraud.
    - If TransactionAmount > 1289.39 and:
      - MerchantID <= 89.0 and CustomerAge > 18.5 and AccountBalance <= 1894.64 → classify as fraud.
      - MerchantID <= 89.0 and CustomerAge > 18.5 and AccountBalance > 1894.64 → classify as fraud.
 
 2. When LoginAttempts > 3.5:
    - If TransactionAmount <= 393.88:
      - If Amount_to_AvgByType_Ratio <= 1.04 and AccountBalance > 10336.47 → classify as fraud.
      - If Amount_to_AvgByType_Ratio <= 1.04 and LoginAttempts > 4.5 → classify as fraud.
    - If TransactionAmount > 393.88:
      - If DeviceID <= 632.5 → classify as fraud.
      - If DeviceID > 632.5 and MerchantID <= 20 → classify as fraud.
 
 All other cases are classified as non-fraudulent.


**Detected Patterns in Flagged Data (compared to the overall dataset, implicitly):**

Based on the columns included in the `anomalies_table` and the feature engineering steps, the ensemble anomaly detection likely flagged transactions that exhibit the following patterns:

1.  **Unusual Transaction Amount:** The `Amount_to_AvgByType_Ratio` feature is likely a strong indicator. Flagged transactions might have:
    *   Significantly higher `TransactionAmount` compared to the average amount for their specific `TransactionType`.
    *   Potentially significantly lower `TransactionAmount` if the ratio is very small (though high amounts are more typical for fraud).
    *   The Z-score outlier detection was performed earlier, suggesting extreme amounts are a potential pattern.
2.  **Unusual Timing:**
    *   **`TimeSinceLastTx`:** Anomalies might occur after unusually long or unusually short periods since the previous transaction for that account (if `PreviousTransactionDate` was available and used). Very short times might indicate rapid, potentially automated activity, while very long times might indicate activity on a dormant account.
    *   **`Hour`:** Certain hours might have a higher proportion of anomalies. For example, transactions occurring very early in the morning or late at night might be more suspicious depending on the typical user behavior.
3.  **Unusual Transaction Duration:** The `TransactionDuration` feature (time difference between transaction date and previous transaction date, although the column name might be slightly misleading given the calculation description) being significantly different from the norm could be a pattern.
4.  **Unusual Device Activity:** The `DeviceTxCount` feature measures the transaction frequency for a specific device. Anomalies might be associated with:
    *   Devices with a very low number of total transactions (potentially a new or unknown device).
    *   Alternatively, devices with an unusually high burst of transactions in a short period (if the count reflects recent activity, though here it seems to be a total count).
5.  **Specific Transaction Types:** While not explicitly summarized in the printouts, certain `TransactionType` values might be disproportionately represented in the anomaly set. For example, international transfers or high-value withdrawals might be more likely to be flagged. You would need to analyze the distribution of `TransactionType` within the `anomalies_table`.
6.  **Specific Locations or Devices:** Similar to transaction types, certain `Location` values or `DeviceID` values might appear more frequently in the anomaly set, suggesting compromised devices or suspicious locations.

To get a more precise understanding of the patterns, you would typically perform a comparative statistical analysis between the `anomalies_table` and a sample of the normal transactions, looking at the mean, median, and distribution of the features and the frequency of categorical values within the anomaly set versus the normal set. The `display(anomalies_table)` output itself allows for a visual inspection of individual anomalous records.

* Anomaly agreement rates:

  * Isolation Forest & DBSCAN: \~87%
  * Isolation Forest & OCSVM: \~84%
  * DBSCAN & OCSVM: \~81%
* Ensemble agreement across all: \~84%

📌 **Conclusion and Recommendation:**

#### Fraud Detection System Summary:

---

1. **Data Exploration:**

   * Analyzed transaction patterns by time, amount, type, and channel
   * Identified initial data characteristics and potential anomalies

2. **Feature Engineering:**

   * Created time-based features (hour, day, weekend flag)
   * Engineered behavioral features (deviation from typical patterns)
   * Added transaction velocity and pattern-based features

3. **Modeling Approach:**

   * Applied multiple unsupervised algorithms (Isolation Forest, DBSCAN, One-Class SVM, LOF)
   * Created ensemble model combining multiple algorithm outputs
   * Implemented cluster analysis to identify transaction segments

4. **Anomaly Characterization:**

   * Identified key features distinguishing anomalies from normal transactions
   * Analyzed temporal patterns of anomalies
   * Created comprehensive risk scoring system

5. **Model Evaluation:**

   * Compared model agreement levels
   * Observed consistent patterns across multiple anomaly detection methods

This unsupervised fraud detection system identifies anomalous transactions based on a majority vote across 3 models. Feature engineering emphasized behavioral timing, transaction types, and device usage. Ensemble learning improves robustness, while the decision tree helps auditors understand *why* a transaction was flagged.
